<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Zeyu Huang</title>
  <meta name="author" content="Zeyu Huang">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <style type="text/css">
    @import url(http://fonts.googleapis.com/css?family=Roboto:400,400italic,500,500italic,700,700italic,900,900italic,300italic,300,100italic,100);
      /* Color scheme stolen from Sergey Karayev */
      a {
      /*color: #b60a1c;*/
      color: #1772d0;
      /*color: #bd0a36;*/
      text-decoration:none;
      }
      a:focus, a:hover {
      color: #f09228;
      text-decoration:none;
      }
      body,td,th,tr,p,a {
      font-family: 'Roboto', sans-serif;
      font-size: 15px;
      font-weight: 300;
      }
      strong {
      font-family: 'Roboto', sans-serif;
      /*font-family: 'Lato', Verdana, Helvetica, sans-serif;*/
      /*font-family: 'Avenir Next';*/
      font-size: 15px;
      font-weight: 400;
      }
      heading {
      /*font-family: 'Lato', Verdana, Helvetica, sans-serif;*/
      font-family: 'Roboto', sans-serif;
      /*font-family: 'Avenir Next';*/
      /*src: url("./fonts/Roboto_Mono_for_Powerline.ttf");*/
      font-size: 24px;
      font-weight: 400;
      }
      papertitle {
      /*font-family: 'Lato', Verdana, Helvetica, sans-serif;*/
      font-family: 'Roboto', sans-serif;
      /*font-family: 'Avenir Next';*/
      /*src: url("./fonts/Roboto_Mono_for_Powerline.ttf");*/
      font-size: 15px;
      font-weight:500;
      }
      name {
      /*font-family: 'Lato', Verdana, Helvetica, sans-serif;*/
      font-family: 'Roboto', sans-serif;
      /*font-family: 'Avenir Next';*/
      font-weight: 400;
      font-size: 32px;
      }
      .one
      {
      width: 160px;
      height: 140px;
      position: relative;
      }
      .two
      {
      width: 160px;
      height: 160px;
      position: absolute;
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
      }
      .fade {
       transition: opacity .2s ease-in-out;
       -moz-transition: opacity .2s ease-in-out;
       -webkit-transition: opacity .2s ease-in-out;
      }
      span.highlight {
          background-color: #ffffd0;
      }
    </style>
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>
<body>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tbody>
      <tr>
        <td>
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <!--<tr onmouseout="headshot_stop()" onmouseover="headshot_start()">-->
            <tbody>
              <tr>
                <td width="75%" valign="middle">
                  <p align="center">
                    <name>Zeyu Huang ÈªÑÊ≥ΩÂÆá</name>
                  </p>

                  <p>
                    I am currently a Ph.D. student in Computer Science supervised by Prof. <a href="https://csse.szu.edu.cn/staff/ruizhenhu/">Ruizhen Hu</a>, working in <a href="https://vcc.tech/">Visual Computing Research Center</a>, <a href="https://en.szu.edu.cn/">Shenzhen University</a>. <br>
                    Before that I got my B.Eng. in Software Engineering from <a href="https://en.szu.edu.cn/">Shenzhen University</a>.
                  </p>

                  <p>
                    I am interested in Computer Graphics, Computer Vision and Robotics, especially on applying deep learning to synthesize graphics contents.
                  </p>

                  <p align="center">
                    <a href="mailto:zeyuhuang97@gmail.com">Email</a> &nbsp/&nbsp
                    <a href="data/zeyuhuang_cv.pdf">CV</a> &nbsp/&nbsp
                    <a href="https://scholar.google.com/citations?user=LXpcIyQAAAAJ">Google Scholar</a> &nbsp/&nbsp
                    <a href="https://github.com/zzilch/">Github</a>
                  </p>
                </td>

                <td width="25%">
                  <a href="images/photo.jpg"><img style="width:100%;max-width:100%" alt="profile photo"
                      src="images/photo.jpg" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Research</heading>
                  <p>
                    I have strong interests in graphics content synthesis. Specifically, my researching projects cover
                    the following topics: 3D Reconstruction, Interaction Generation, Object Manipulation, Floorplan
                    Generation, etc.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>

              <tr onmouseout="" onmouseover="">
                <td width="25%">
                  <div class="one">
                    <img src='images/sscf.png' width="190">
                  </div>
                </td>
                <td valign="top" width="75%">
                  <a href="">
                    <papertitle>Spatial and Surface Correspondence Field for Interaction Transfer</papertitle>
                  </a>
                  <strong>Zeyu Huang</strong>,
                  <a href="">Honghao Xu</a>
                  <a href="https://brotherhuang.github.io/">Haibin Huang</a>,
                  <a href="http://www.chongyangma.com/">Chongyang Ma</a>,
                  <a href="http://vcc.tech/~huihuang">Hui Huang</a>,
                  <a href="https://csse.szu.edu.cn/staff/ruizhenhu/">Ruizhen Hu</a>
                  <br>
                  <em>SIGGRAPH</em>, 2024
                  <br>
                  <p> In this paper, we introduce a new method for the task of interaction transfer. </p>
                </td>
              </tr>

              <tr onmouseout="" onmouseover="">s
                <td width="25%">
                  <div class="one">
                    <img src='images/dina.jpg' width="190">
                  </div>
                </td>
                <td valign="top" width="75%">
                  <a>
                    <papertitle>DINA: Deformable INteraction Analogy</papertitle>
                  </a>
                  <br>
                  <br>
                  <strong>Zeyu Huang*</strong>,
                  <a href="https://scholar.google.com/citations?user=oyklclQAAAAJ&hl">Sisi Dai</a>
                  <a href="https://kevinkaixu.net/">Kai Xu</a>,
                  <a href="https://www.cs.sfu.ca/~haoz/">Hao Zhang</a>,
                  <a href="http://vcc.tech/~huihuang">Hui Huang</a>,
                  <a href="https://csse.szu.edu.cn/staff/ruizhenhu/">Ruizhen Hu</a>
                  <br>
                  <em>GMOD</em>, 2024 
                  <br>
                  <a href="https://www.sciencedirect.com/science/article/pii/S1524070324000055">paper</a> /
                  <p></p>
                  <p> A means to generate interactions between two 3D objects with a descriptive and robust interaction representation. </p>
                </td>
              </tr>

              <!-- <tr onmouseout="" onmouseover="" bgcolor="#ffffd0"></tr> -->
              <tr onmouseout="" onmouseover="">
                <td width="25%">
                  <div class="one">
                    <img src='images/aro_net.png' width="190">
                  </div>
                </td>
                <td valign="top" width="75%">
                  <a href="https://aro-net.github.io/">
                    <papertitle>ARO-Net: Learning Implicit Fields from Anchored Radial Observations</papertitle>
                  </a>
                  <br>
                  <a href="yizhiwang96.github.io">Yizhi Wang*</a>,
                  <strong>Zeyu Huang</strong>,
                  <a href="https://faculty.runi.ac.il/arik/site/index.asp">Ariel Shamir</a>,
                  <a href="http://vcc.tech/~huihuang">Hui Huang</a>,
                  <a href="https://www.cs.sfu.ca/~haoz/">Hao Zhang</a>,
                  <a href="https://csse.szu.edu.cn/staff/ruizhenhu/">Ruizhen Hu</a>
                  <br>
                  <em>CVPR</em>, 2023
                  <br>
                  (*equal contribution)
                  <br>
                  <a href="https://aro-net.github.io/">project page</a> /
                  <a href="https://arxiv.org/abs/2212.10275">arXiv</a> /

                  <a href="https://github.com/yizhiwang96/ARO-Net">code</a> /
                  <a href="https://www.youtube.com/watch?v=RVoOkgbi9lk&ab_channel=YizhiWang">video</a>
                  <p></p>
                  <p>A novel shape encoding for learning neural field representation of shapes that is category-agnostic
                    and generalizable amid significant shape variations.</p>
                </td>
              </tr>

              <tr onmouseout="" onmouseover="">
                <td width="25%">
                  <div class="one">
                    <img src='images/nift.jpg' width="190">
                  </div>
                </td>
                <td valign="top" width="75%">
                  <a href="https://vcc.tech/research/2023/NIFT">
                    <papertitle>NIFT: Neural Interaction Field and Template for Object Manipulation</papertitle>
                  </a>
                  <br>
                  <strong>Zeyu Huang</strong>,
                  <a href="https://github.com/Juzhan">Juzhan Xu</a>,
                  <a href="https://scholar.google.com/citations?user=oyklclQAAAAJ&hl">Sisi Dai</a>,
                  <a href="https://kevinkaixu.net/">Kai Xu</a>,
                  <a href="https://www.cs.sfu.ca/~haoz/">Hao Zhang</a>,
                  <a href="http://vcc.tech/~huihuang">Hui Huang</a>,
                  <a href="https://csse.szu.edu.cn/staff/ruizhenhu/">Ruizhen Hu</a>
                  <br>
                  <em>ICRA</em>, 2023
                  <br>
                  <a href="https://vcc.tech/research/2023/NIFT">project page</a> /
                  <a href="https://arxiv.org/abs/2210.10992">arXiv</a> /

                  <a href="https://github.com/zzilch/NIFT">code</a> /
                  <a href="https://www.youtube.com/watch?v=ek-XhVMSZhg&t=5s">video</a>
                  <p></p>
                  <p>A descriptive and robust interaction representation of object manipulations to facilitate imitation learning.</p>
                </td>
              </tr>

              <tr onmouseout="" onmouseover="">
                <td width="25%">
                  <div class="one">
                    <img src='images/graph2plan.jpg' width="190">
                  </div>
                </td>
                <td valign="top" width="75%">
                  <a href="https://vcc.tech/research/2020/Graph2Plan">
                    <papertitle>Graph2Plan: Learning Floorplan Generation from Layout Graphs</papertitle>
                  </a>
                  <br>
                  <a href="https://csse.szu.edu.cn/staff/ruizhenhu/">Ruizhen Hu</a>,
                  <strong>Zeyu Huang</strong>,
                  <a href="https://github.com/HanHan55">Yuhan Tang</a>,
                  <a href="https://people.scs.carleton.ca/~olivervankaick/">Oliver van Kaick</a>,
                  <a href="https://www.cs.sfu.ca/~haoz/">Hao Zhang</a>,
                  <a href="http://vcc.tech/~huihuang">Hui Huang</a>
                  
                  <br>
                  <em>SIGGRAPH</em>, 2020
                  <br>
                  <a href="https://vcc.tech/research/2020/Graph2Plan">project page</a> /
                  <a href="https://arxiv.org/abs/2004.13204">arXiv</a> /

                  <a href="https://github.com/HanHan55/Graph2plan">code</a> /
                  <a href="https://www.youtube.com/watch?v=GUasDAFTY8E">video</a>
                  <p></p>
                  <p>A learning framework for automated floorplan generation which combines generative modeling using deep neural networks and user-in-the-loop designs to enable human users to provide sparse design constraints.</p>
                </td>
              </tr>

            </tbody>
          </table>


          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  <br>
                  Template adapted from <a href="https://yizhiwang96.github.io/">Yizhi</a> and <a href="https://jonbarron.info/">Jon</a>.
                </p>
              </td>
            </tr>
          </tbody></table>   

      </td>
    </tr>
  </table>
</body>

</html>
